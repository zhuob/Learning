---
title: "Classification"
output: 
  html_notebook:
    number_sections: true
---
  

# Decision Trees
  + If the target variable is discrete, then it's called *classification tree*, whereas for continuous outcome, it's called *regression tree*.
  + **CART** (Classification And Regression Tree) analysis is an umbrella term used for both of the above procedures.
  + Algorithm for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items. The "best" split is decided by [metrics](https://en.wikipedia.org/wiki/Decision_tree_learning) that measures the homogeneity of the target variable within the subsets. For a set of $J$ classes, suppose $i\in\{1, 2, \ldots, J\}$, and let $p_i$ be the fraction of items labeled with class $i$ in the set.
    + Gini Impurity: $Gini(E) = \sum_{i=1}^Jp_i(1-p_i) = 1- \sum_{i=1}^Jp_i^2$
    + Information Gain: $H(E) = -\sum_{i=1}^Jp_i\log_2p_i$
 
#  Random Forest
  + Reference: **Element of Statistical Learning**
  + Random forest: decision tree --> tree bagging --> random forest
      + random forests differs in one way from tree bagging. They use a modified tree learning algorithm that selects, at each candidate split in the learning process,  **a random subset of the features**. 
       + the reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable, these features will be selected in many of the B trees, causing them to be correlated. 
      + Typically, for a classification problem with $p$ features, $\sqrt{p}$ (rounded down) features are used in each split. For regression problems, the inventors recommend $p/3$ (round down) with a minimum node size of 5 as the default.
 
  + Algorithm
    + From $b=1$ to B
        (a) Draw a bootstrapped sample $Z^{\ast}$ of size $N$ from the training set.
        (b) Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until node size is reached. 
            + randomly select $m$ variables from the $p$ variables. 
            + pick the best variable/split-point among the $m$.
            + split the node into two daughter nodes. 
    + Output the emsemble of trees $\{T_1, \ldots, T_B\}$
    + Then the prediction at a new point $x$
        + Regression: $\hat{f}^B(x) = \frac{1}{B}\sum_{i=1}^BT_i(x)$
        + Classification: the majority vote.

 + Feature
     + Simpler to train and tune
 
     + Because of random selection of variables, the trees trained are identically distributed. This is different from boosting methods, where trees are growed in an adaptive way to reduce bias, and thus are not i.d.
     + Out of bag sample (OOB) can play a similar role as N-fold cross-validation. For each observation $(x_i, y_i)$, construct the random forest predictor by averaging those trees that is built without this sample. When OOB error stabilize, the training can be terminated. 
    + OOB can measure variable importance, i.e., the prediction strength of each variable. The importance of $j$th variable is measured by the decreased accuracy when the values of this variable are randomly permuted in the OOB samples. This process will be done for each of the trees, and resulting accuracy is averaged. 
 + R package is [*randomForest*](https://cran.r-project.org/web/packages/randomForest/index.html)
 
 
# K-Nearest Means 
  
  + Reference: 
    + https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm 
    + **Element of Statistical Learning**.
  + Algorithm
    + calculate the pairwise distances (<span style = "color:red">Euclidian distance</span> for continuous variable, <span style = "color:red">hamming distance [overlap metric]</span> for discrete variable)
    + for sample $i$, get its $k$-nearest neighbors
    + tabulate and see the majority class label of the $k$ neighbors
    + assign label to sample $i$.
    + repeat step 1-step 4 until all samples are classified.
  + Features:
    + $k$-nearest neighbor is often successful where the decision boundary is irregular.
    + One drawback of nearest-neighbor rules in general is the computational load, both in finding the neighbors and storing the entire training set
 + R package is [*FastKNN*](https://cran.r-project.org/web/packages/FastKNN/index.html)
 
# Naive Bayes

  + Often called Bayes classifier, whose name comes from the Bayes rule
  $P(y|\mathbf X) = \frac{P(y)\cdot P(\mathbf X|y)}{P(\mathbf X)} = \frac{P(y)\cdot P(\mathbf X|y)}{\sum_{y}P(\mathbf X, y)}$
  + Assumptions: Features are independent of each other given label 
  + *conditional independence*, $p(x_1, x_2|y) = p(x_1|y)*p(x_2|y)$, or equivalently, $p(x_1|y) = p(x_1|x_2, y)$
  + under this assumption, we have $p(\mathbf x|y) = \prod_{i=1}^kp(x_i|y)$
  + Bayes estimator can be used if for some specific $x_i$, $p(x_i|y)=0$, by maximizing posterior estimate.
  + Algorithm
    + Given training data
    + Learn $P(y)$
    + Learn $P(\mathbf x|y=1), P(\mathbf x|y=2), \ldots, P(\mathbf x|y=k)$
    + Compute $P(y|\mathbf x) = P(y)\cdot P(\mathbf X|y)/\sum_{y}P(\mathbf X, y)$
    + Predict with decision theory or use $\arg \max_YP(y|\mathbf x)$
 
 
# Emsemble Methods

  + Bagging stands for **B**ootstrapped **Agg**regat**ing**
      + A learning algorithm is unstable if small changes in the training data can produce large changes in the output hypothesis. $\rightarrow$ high variance.
      + Bagging have little benefits when used with stable learning algorithm.
      + Bagging works best for high variance and low bias algorithm.
  + Boosting
      + looking at errors from previous classifiers to decide what to focus on the next iteration over data. 
      + The main idea of boosting is to add new models to the ensemble sequentially. 
      + more weights on 'hard' examples --- those we have committed mistakes in the previous iteration.
  + Adaboost algorithm
    + input 
        + Learn - Base learning algorithm
        + S --- set of N labeled training examples.
    + output 
        + $H = [H_1, \cdots, H_L, weighted~votes~(\alpha_1, \ldots, \alpha_L)]$
    + Steps: Let $D_l$ be the distribution of round $l$ for the training set. 
        + initialize $D_1(i) = 1/N$ (i.e., uniform distribution)
        + For $l = 1, \ldots, L$, Do
            + $h_l = learn (S, D_l)$
            + $\epsilon_l = error(h_l, S, D_l)$
            + $\alpha_l = \frac{1}{2}\ln\frac{1-\epsilon_l}{\epsilon_l}$
            + $D_{l+1}(i) = D_l(i)\times e^{\alpha_i}$ if $h_l(x_i) \neq y_i$, else               $D_{l+1}(i) = D_l(i)\times e^{-\alpha_i}$ 
            
    + Note that $\epsilon_l < 0.5$ implies $\alpha_l > 0$, which means weight decreases for correct examples. 
    + Feature:
        + Boosting is often, though not always, robust to overfitting (Schapire 1989).
        +  Test error continues to decrease even after training error goes to 0.
        + Sensitive to noise and outliers, as compared to bagging that is robust to outliers.
        
        
# Support Vector Machine        
        
        
# Gradient Boosting Machines

## References

 +  J.H. Friedman (2001). "Greedy Function Approximation: A Gradient Boosting Machine," Annals of Statistics 29(5):1189-1232.
 + Friedman, J., Hastie, T., and Tibshirani, R. (2000). Special invited paper. additive logistic regression: A statistical view of boosting. Annals of statistics, pages 337â€“374.
 + Schapire, R. E. and Freund, Y. (2012). Boosting: Foundations and Algorithms. MIT Press.
 + http://www.chengli.io/tutorials/gradient_boosting.pdf
 + [Natekin, Alexey, and Alois Knoll. "Gradient boosting machines, a tutorial." Frontiers in neurorobotics 7 (2013): 21.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)
 + [Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. "Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)." The annals of statistics 28.2 (2000): 337-407.](https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf for more detail)
 + `R` package [`gbm`](https://cran.r-project.org/web/packages/gbm/)
 
## Method 

(based on Natekin and Alois, 2013) Consider the data set $(\mathbf x, y)_{i = 1}^N$, where $\mathbf x = (x_1, \ldots, x_d)$ refers to the predictors, and $y$ to the corresponding labels of the response variable. The goal is to construc the unknown functional dependence $\mathbf{x}\overset{f}{\rightarrow}y$ with estimate $\hat{f}(\mathbf{x})$, such that some specific loss function $\Psi(y, f)$ is minimized, $\\ \hat{f}(x) = y, \\ 
  \hat{f}(x) = \arg\min_{f(x)}E_x[E_y[\Psi(y, f(x))]|x]$,where $E_y[\Psi(y, f(x))]$ is the expected loss and $E_x[~]$ is the expectation over all dataset.  We can restrict the function search space to a parametric family of functions $f(x, \theta)$, which will change the function optimization into parametric estimation 

$hat{f}(x) = f(x, \hat{\theta}), \\ \hat{\theta} = \arg\min_{\theta}E_x[E_y[\Psi(y, f(x, \theta))]|x], \label{E1}$
 Given $M$ iterations, the parameter estimates can be written in the incremental form 
 $\hat{\theta} = \sum_{i =1}^M\hat{\theta}_i$
 
  In the boosting methods, the optimization is held out in the function space. That is 
  \[\hat{f}(x) = \hat{f}^M(x) = \sum_{i = 0}^M\hat{f}_i(x)\]
  where $M$ is the number of iterations $\hat{f}_0$ is the initial guess and $\{\hat{f}_i\}_{i=1}^M$ are the function increaments, also called "boosts." For the function estimate at $t$-th iteration, the optimization is defined as 
  \[\hat{f}_t\leftarrow \hat{f}_{t-1} +\rho_th(x, \theta_t), \\
  (\rho_t, \theta_t) = \arg\min_{\rho, \theta}\sum_{i = 1}^N\Psi(y_i, \hat{f}_{t-1}) + \rho h(x_i, \theta)\]
 The simplest and most frequently used procedure is steepest gradient descent. Suppose we have a base-learner $h(x,\theta)$, it is proposed that the new function $h(x,\theta_t)$ be most parallel to the negative gradient $\{g_t(x_i)\}_{i =1}^N$ with 
 \[g_t(\mathbf{x}) = E_y[\frac{\partial \Psi(y, f(x))}{\partial f(x)}|x]_{f(x)= \hat{f}^{t-1}(x)}\]
Therefore, teh exact form of the derived algorithm with all the corresponding formulas will heavily depend on the design choice of $\Psi(y, f)$ and $h(x, \theta)$.

### Loss functions
  + Continuous response, $y\in R$.
    + Gaussian $L_2$ loss function
    + Laplace $L_1$ loss function
    + Huber loss function $\delta$ specified
    + Quantile loss function, $\alpha$ specified
  + Categorical response, $y\in\{0, 1\}$
    + Binomial loss function
    + Adaboost loss function 
  + Other families of response variable
    + Loss functions for survival models
    + Loss functions for counts data
    + Custom loss functions

### Base learner
  + Linear models
    + Ordinary linear regression
    + Ridge penalized linear regression
    + Random effects
  + Smooth models
    + P-splines
    + Radial basis functions
  + Decision trees
    + Decision tree stumps
    + Decision trees with arbitrary interaction depth
  + Other models
    + Markov Random Fields
    + Wavelets
    + Custom base-learner functions

## Relative Influence
This is the available in the decision-tree ensembles. Let's define the influence of the variable $j$ in a single tree $T$. Consider the tree has $L$ splits, therefore we are looking for all the non-terminal nodes from the root to the $L-1$ level of the tree. Then the variable influence is defined as 
\[Influence_j(T) = \sum_{i =1 }^{L-1}I_i^21(S_i = j)\]
which is based on the number of times a variable is selected for splitting. The term $I_i^2$ is the emprirical squared improvement when splitting variable $S_i$ is $j$. To obtain the overall influence of variable $j$, this metric should be averaged over all trees
\[Influence_j = \frac{1}{M}\sum_{i=1}^MInfluence_j(T_i)\]
Several notes:

  + The influences are further standardized so that they sum up to 100\%.
  + Influences do not provide any explanations about how the variable actually affect the response.
  + The resulting variables can be used for variable selections.
  
## Drawbacks

  + Memory consumption: The cost of storing a predictive model depends on the number of boosting iterations.
  + Evaluation speed: to use the fitted GBM to obtain predictions, one has to evaluate all the base-learners in the ensemble.
  + Not parallelization friendly: the learning procedure is sequential.
 
## Application
  An example is boosted logistic regression. 



  
  
 
