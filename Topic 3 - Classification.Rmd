---
title: "Classification"
output: 
  html_notebook:
    number_sections: true
---
  

# Decision Trees
  + If the target variable is discrete, then it's called *classification tree*, whereas for continuous outcome, it's called *regression tree*.
  + **CART** (Classification And Regression Tree) analysis is an umbrella term used for both of the above procedures.
  + Algorithm for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items. The "best" split is decided by [metrics](https://en.wikipedia.org/wiki/Decision_tree_learning) that measures the homogeneity of the target variable within the subsets. For a set of $J$ classes, suppose $i\in\{1, 2, \ldots, J\}$, and let $p_i$ be the fraction of items labeled with class $i$ in the set.
    + Gini Impurity: $Gini(E) = \sum_{i=1}^Jp_i(1-p_i) = 1- \sum_{i=1}^Jp_i^2$
    + Information Gain: $H(E) = -\sum_{i=1}^Jp_i\log_2p_i$
 
#  Random Forest
  + Reference: **Element of Statistical Learning**
  + Random forest: decision tree --> tree bagging --> random forest
      + random forests differs in one way from tree bagging. They use a modified tree learning algorithm that selects, at each candidate split in the learning process,  **a random subset of the features**. 
       + the reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable, these features will be selected in many of the B trees, causing them to be correlated. 
      + Typically, for a classification problem with $p$ features, $\sqrt{p}$ (rounded down) features are used in each split. For regression problems, the inventors recommend $p/3$ (round down) with a minimum node size of 5 as the default.
 
  + Algorithm
    + From $b=1$ to B
        (a) Draw a bootstrapped sample $Z^{\ast}$ of size $N$ from the training set.
        (b) Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until node size is reached. 
            + randomly select $m$ variables from the $p$ variables. 
            + pick the best variable/split-point among the $m$.
            + split the node into two daughter nodes. 
    + Output the emsemble of trees $\{T_1, \ldots, T_B\}$
    + Then the prediction at a new point $x$
        + Regression: $\hat{f}^B(x) = \frac{1}{B}\sum_{i=1}^BT_i(x)$
        + Classification: the majority vote.

 + Feature
     + Simpler to train and tune
 
     + Because of random selection of variables, the trees trained are identically distributed. This is different from boosting methods, where trees are growed in an adaptive way to reduce bias, and thus are not i.d.
     + Out of bag sample (OOB) can play a similar role as N-fold cross-validation. For each observation $(x_i, y_i)$, construct the random forest predictor by averaging those trees that is built without this sample. When OOB error stabilize, the training can be terminated. 
    + OOB can measure variable importance, i.e., the prediction strength of each variable. The importance of $j$th variable is measured by the decreased accuracy when the values of this variable are randomly permuted in the OOB samples. This process will be done for each of the trees, and resulting accuracy is averaged. 
 + R package is [*randomForest*](https://cran.r-project.org/web/packages/randomForest/index.html)
 
 
# K-Nearest Means 
  
  + Reference: https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm and **Element of Statistical Learning**.
  + Algorithm
    + calculate the pairwise distances (<span style = "color:red">Euclidian distance</span> for continuous variable, <span style = "color:red">hamming distance [overlap metric]</span> for discrete variable)
    + for sample $i$, get its $k$-nearest neighbors
    + tabulate and see the majority class label of the $k$ neighbors
    + assign label to sample $i$.
    + repeat step 1-step 4 until all samples are classified.
  + Features:
    + $k$-nearest neighbor is often successful where the decision boundary is irregular.
    + One drawback of nearest-neighbor rules in general is the computational load, both in finding the neighbors and storing the entire training set
 + R package is [*FastKNN*](https://cran.r-project.org/web/packages/FastKNN/index.html)
 
# Naive Bayes

  + Often called Bayes classifier, whose name comes from the Bayes rule
  $P(y|\mathbf X) = \frac{P(y)\cdot P(\mathbf X|y)}{P(\mathbf X)} = \frac{P(y)\cdot P(\mathbf X|y)}{\sum_{y}P(\mathbf X, y)}$
  + Assumptions: Features are independent of each other given label 
  + *conditional independence*, $p(x_1, x_2|y) = p(x_1|y)*p(x_2|y)$, or equivalently, $p(x_1|y) = p(x_1|x_2, y)$
  + under this assumption, we have $p(\mathbf x|y) = \prod_{i=1}^kp(x_i|y)$
  + Bayes estimator can be used if for some specific $x_i$, $p(x_i|y)=0$, by maximizing posterior estimate.
  + Algorithm
    + Given training data
    + Learn $P(y)$
    + Learn $P(\mathbf x|y=1), P(\mathbf x|y=2), \ldots, P(\mathbf x|y=k)$
    + Compute $P(y|\mathbf x) = P(y)\cdot P(\mathbf X|y)/\sum_{y}P(\mathbf X, y)$
    + Predict with decision theory or use $\arg \max_YP(y|\mathbf x)$
 
 
# Emsemble Methods

  + Bagging stands for **B**ootstrapped **Agg**regat**ing**
      + A learning algorithm is unstable if small changes in the training data can produce large changes in the output hypothesis. $\rightarrow$ high variance.
      + Bagging have little benefits when used with stable learning algorithm.
      + Bagging works best for high variance and low bias algorithm.
  + Boosting
      + looking at errors from previous classifiers to decide what to focus on the next iteration over data. 
      + more weights on 'hard' examples --- those we have committed mistakes in the previous iteration.
  + Adaboost algorithm
    + input 
        + Learn - Base learning algorithm
        + S --- set of N labeled training examples.
    + output 
        + $H = [H_1, \cdots, H_L, weighted~votes~(\alpha_1, \ldots, \alpha_L)]$
    + Steps: Let $D_l$ be the distribution of round $l$ for the training set. 
        + initialize $D_1(i) = 1/N$ (i.e., uniform distribution)
        + For $l = 1, \ldots, L$, Do
            + $h_l = learn (S, D_l)$
            + $\epsilon_l = error(h_l, S, D_l)$
            + $\alpha_l = \frac{1}{2}\ln\frac{1-\epsilon_l}{\epsilon_l}$
            + $D_{l+1}(i) = D_l(i)\times e^{\alpha_i}$ if $h_l(x_i) \neq y_i$, else               $D_{l+1}(i) = D_l(i)\times e^{-\alpha_i}$ 
            
    + Note that $\epsilon_l < 0.5$ implies $\alpha_l > 0$, which means weight decreases for correct examples. 
    + Feature:
        + Boosting is often, though not always, robust to overfitting (Schapire 1989).
        +  Test error continues to decrease even after training error goes to 0.
        + Sensitive to noise and outliers, as compared to bagging that is robust to outliers.
        
        
# Support Vector Machine        
        


# Metric for model evaluation

## The Kappa statistic 
    
  + Reference: http://www1.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf
  + Accuracy and Precision: in the above reference, there's an interesting figure showing the difference between the two concept. If we actually hit the bull's eye, we are *accurate* (representing agreement with the gold standard); if our shots land together, we have good *precision* (good reliability); if our shots land together and hit the bull's eye, we are both precise and accurate. 
  + Kappa measures how different is the observed agreement from the expected agreement. Kappa = $\frac{p_0 - p_e}{1-p_e}$ where $p_0$ is the observed agreement and $p_e$ is the expected agreement.
  + Kappa is standardized to lie in between 0 and 1, where 1 is perfect agreement, 0 is exactly what would be expected by chance, and negative values indicate agreement less by chance.
  + Weighted Kappa: Assigns less weight to agreement as categories are further apart

  + Feature:
    +  Interpretation of Kappa
      
      Kappa   | Agreement 
      ----  |    ------
       < 0 Less  | than chance agreement 
      0.01–0.20  | Slight agreement   
      0.21– 0.40 | Fair agreement    
      0.41–0.60  |Moderate agreement  
      0.61–0.80  |Substantial agreement  
      0.81–0.99  |Almost perfect agreement  

    + Kappa may not be reliable for rare observations: high agreement does not guarantee higher kappa. For example, if $p_0 = 84/99$, $p_1 = 0.85$,  that gives an agreement of 85\%, but a kappa value of merely 0.01. 
    
